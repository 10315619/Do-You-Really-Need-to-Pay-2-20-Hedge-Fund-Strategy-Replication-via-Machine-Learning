{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from __future__ import print_function, division"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle\n", "from datetime import datetime\n", "from random import randint"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pandas as pd\n", "from keras.layers import Input, Dense, LeakyReLU, LSTM, LayerNormalization\n", "from keras.models import Sequential, Model\n", "from keras.optimizers import RMSprop, Adam"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import keras.backend as K"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Read-in cleaned data"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from sklearn.preprocessing import MinMaxScaler"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def read_csv(loc, date=True):\n", "    df = pd.read_csv(loc)\n", "    if date:\n", "        df['Date'] = pd.to_datetime(df['Date'])\n", "        df.set_index('Date', inplace=True)\n", "    return df"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def dic_read(loc):\n", "    a_file = open(loc, \"rb\")\n", "    output = pickle.load(a_file)\n", "    return output"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def set_seed(seed_value=123):\n", "    import os\n", "    import random\n", "    import tensorflow as tf\n", "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n", "    np.random.seed(seed_value)\n", "    random.seed(seed_value)\n", "    tf.random.set_seed(seed_value)\n", "    from keras import backend as K\n", "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n", "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n", "    tf.compat.v1.keras.backend.set_session(sess)\n", "    K.set_session(sess)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def random_sampling(dataset, n_sample, window):\n", "    '''\n", "    implicitly assuming there is no calendar effect.\n", "    :param dataset: np.ndarray\n", "    :param n_sample:\n", "    :param window:\n", "    :return:\n", "    '''\n", "    isinstance(dataset, np.ndarray)\n", "    step = 0\n", "    res = []\n", "    while step < n_sample:\n", "        step += 1\n", "        randidx = randint(0, dataset.shape[0] - window)\n", "        res.append(dataset[randidx:window + randidx])\n", "    # label as real data\n", "    # label = np.ones(n_sample)\n", "    # return np.array(res), label\n", "    return np.array(res)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["set_seed()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["hfd = read_csv('../cleaned_data/hfd.csv')\n", "factor_etf_data = read_csv('../cleaned_data/factor_etf_data.csv')\n", "hfd_fullname = dic_read('../cleaned_data/hfd_fullname.pkl')\n", "factor_etf_name = dic_read('../cleaned_data/factor_etf_name.pkl')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_data_name = {**factor_etf_name, **hfd_fullname}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = factor_etf_data.join(hfd)\n", "data_scaler = MinMaxScaler()\n", "data = data_scaler.fit_transform(dataset)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dataset = random_sampling(data, 1000, 48)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class WGAN():\n", "    def __init__(self, dataset):\n", "        isinstance(dataset, np.ndarray)\n", "        self.X_train = dataset\n", "        self.ts_length = self.X_train.shape[1]\n", "        self.ts_feature = self.X_train.shape[2]\n", "        self.ts_shape = (self.ts_length, self.ts_feature)\n", "        self.latent_shape = (self.ts_length, self.ts_feature)\n\n", "        # Following parameter and optimizer set as recommended in paper\n", "        self.n_critic = 5\n", "        self.clip_value = 0.01\n", "        optimizer = RMSprop(learning_rate=0.00005)\n\n", "        # Build and compile the critic\n", "        self.critic = self.build_critic()\n", "        self.critic.compile(loss=self.wasserstein_loss,\n", "                            optimizer=optimizer,\n", "                            metrics=['accuracy'])\n\n", "        # Build the generator\n", "        self.generator = self.build_generator()\n\n", "        # The generator takes noise as input and generated imgs\n", "        z = Input(shape=(self.ts_length, self.ts_feature,))\n", "        ts = self.generator(z)\n\n", "        # For the combined model we will only train the generator\n", "        self.critic.trainable = False\n\n", "        # The critic takes generated images as input and determines validity\n", "        valid = self.critic(ts)\n\n", "        # The combined model  (stacked generator and critic)\n", "        self.combined = Model(z, valid)\n", "        self.combined.compile(loss=self.wasserstein_loss,\n", "                              optimizer=optimizer,\n", "                              metrics=['accuracy'])\n", "    def wasserstein_loss(self, y_true, y_pred):\n", "        return K.mean(y_true * y_pred)\n", "    def build_generator(self):\n", "        model = Sequential(\n", "            [\n", "                Dense(100, input_shape=self.latent_shape, activation='sigmoid', ),\n", "                LeakyReLU(alpha=.2),\n", "                LayerNormalization(),\n", "                Dense(100, activation='sigmoid'),\n", "                LeakyReLU(alpha=.2),\n", "                LayerNormalization(),\n", "                Dense(self.ts_feature)\n", "            ])\n", "        model.summary()\n", "        noise = Input(shape=(self.ts_length, self.ts_feature,))\n", "        img = model(noise)\n", "        return Model(noise, img)\n", "    def build_critic(self):\n", "        model = Sequential(\n", "            [\n", "                Dense(100, input_shape=self.ts_shape, activation=None, ),\n", "                LeakyReLU(alpha=0.2),\n", "                LayerNormalization(),\n", "                Dense(100, activation=None),\n", "                LeakyReLU(alpha=0.2),\n", "                LayerNormalization(),\n", "                Dense(1)  # we dont do sigmoid activation because critic output is supposed to be 1,-1\n", "            ]\n", "        )\n", "        model.summary()\n", "        ts = Input(shape=self.ts_shape)\n", "        validity = model(ts)\n", "        return Model(ts, validity)\n", "    def train(self, epochs, batch_size=128, sample_interval=50):\n\n", "        # Adversarial ground truths\n", "        valid = -np.ones((batch_size, 1))\n", "        fake = np.ones((batch_size, 1))\n", "        for epoch in range(epochs):\n", "            for _ in range(self.n_critic):\n", "                # ---------------------\n", "                #  Train Discriminator\n", "                # ---------------------\n", "                # Select a random batch of images\n", "                idx = np.random.randint(0, self.X_train.shape[0], batch_size)\n", "                imgs = self.X_train[idx]\n", "                # Sample noise as generator input\n", "                noise = np.random.normal(0, 1, (batch_size, self.ts_length, self.ts_feature))\n", "                # Generate a batch of new images\n", "                gen_imgs = self.generator.predict(noise)\n", "                # Train the critic\n", "                d_loss_real = self.critic.train_on_batch(imgs, valid)\n", "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n", "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n", "                # Clip critic weights\n", "                for l in self.critic.layers:\n", "                    weights = l.get_weights()\n", "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n", "                    l.set_weights(weights)\n\n", "            # ---------------------\n", "            #  Train Generator\n", "            # ---------------------\n", "            g_loss = self.combined.train_on_batch(noise, valid)\n\n", "            # Plot the progress\n", "            print(\"%d [D loss: %f] [G loss: %f]\" % (epoch, 1 - d_loss[0], 1 - g_loss[0]))\n", "        time_now = datetime.now().strftime(\"%Y%m%d_%H-%M-%S\")\n", "        self.generator.compile(optimizer=RMSprop(learning_rate=0.00005),\n", "                               loss='binary_crossentropy')  # without compile, we cannot save model. Formality\n", "        self.generator.save(f'./trained_generator/WGAN{time_now}.h5')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    wgan = WGAN(dataset)\n", "    wgan.train(epochs=5000, batch_size=32, sample_interval=50)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}